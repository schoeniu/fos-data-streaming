{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Command for convert .ipynb file to .py:\n",
    "#jupyter nbconvert stream.ipynb --to python"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KAFKA_TOPIC_NAME = \"producedEvents\"\n",
    "KAFKA_TOPIC_NAME_OUT = \"availableDBEntries\"\n",
    "KAFKA_BOOTSTRAP_SERVER = os.environ.get(\"BOOTSTRAP_SERVERS\", \"172.20.0.3:9092\")\n",
    "SPARK_MASTER_URL = \"spark://spark:7077\"\n",
    "TIME_WINDOW = os.environ.get(\"PRODUCER_DATA_SEC_PER_REAL_SEC\", \"1\")+\" seconds\"\n",
    "TIME_TRIGGER = \"1 second\"\n",
    "print(os.environ.get(\"PRODUCER_DATA_SEC_PER_REAL_SEC\", \"\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set submit args. Only needed when executed from jupyter notebook.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.postgresql:postgresql:42.5.1  pyspark-shell'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Initialize Spark session\n",
    "sparkSession = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(SPARK_MASTER_URL) \\\n",
    "        .appName(\"Spark Streaming\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sparkSession.sparkContext.setLogLevel(\"ERROR\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Read events from Kafka producedEvents topic\n",
    "inDf = (\n",
    "        sparkSession.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "        .option(\"subscribe\", KAFKA_TOPIC_NAME)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .option(\"includeTimestamp\", True)\n",
    "        .load()\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Transform dataframe to an easily usable one\n",
    "valueDf = inDf.selectExpr(\"CAST(value as STRING)\")\n",
    "\n",
    "spl = split(valueDf['value'], ',')\n",
    "baseDf = valueDf.withColumn('eventTime', spl.getItem(0)) \\\n",
    "             .withColumn('eventType', spl.getItem(1)) \\\n",
    "             .withColumn('productID', spl.getItem(2)) \\\n",
    "             .withColumn('price', spl.getItem(6)) \\\n",
    "             .drop('value')\n",
    "baseDf = baseDf.withColumn(\"eventTime\", regexp_replace(\"eventTime\", \"\\\"\", \"\")) \\\n",
    "                .withColumn(\"eventTime\", regexp_replace(\"eventTime\", \" UTC\", \".000\")) \\\n",
    "                .withColumn(\"eventTime\", to_timestamp(\"eventTime\"))\n",
    "\n",
    "baseDf = baseDf.withColumn(\"price\", regexp_replace(\"price\", \"\\.\", \"\"))\n",
    "baseDf = baseDf.withColumn(\"price\", baseDf[\"price\"].cast(IntegerType()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Query for aggregating metrics over eventTime window\n",
    "queryDf = baseDf.withWatermark(\"eventTime\", TIME_WINDOW) \\\n",
    "    .groupBy(window(baseDf.eventTime, TIME_WINDOW,TIME_WINDOW)) \\\n",
    "    .agg(count(baseDf.eventType).alias('nr_of_events'),\n",
    "         count(when(baseDf.eventType == 'view', baseDf.productID)).alias('nr_items_viewed'),\n",
    "         count(when(baseDf.eventType == 'cart', baseDf.productID)).alias('nr_items_put_in_cart'),\n",
    "         count(when(baseDf.eventType == 'purchase', baseDf.productID)).alias('nr_items_sold'),\n",
    "         sum(when(baseDf.eventType == 'purchase', baseDf.price)).alias('value_items_sold_in_cent')\n",
    "        )\\\n",
    "     .na.fill(value=0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Define function for sending data to PostgreSQL DB and Kafka\n",
    "def sink(data_frame, batch_id):\n",
    "    #Prepare PostgreSQL DB settings\n",
    "    dbname = 'postgres'\n",
    "    dbuser = 'postgres'\n",
    "    dbpass = 'postgres'\n",
    "    dbhost = '172.20.0.6'\n",
    "    dbport = '5432'\n",
    "\n",
    "    url = \"jdbc:postgresql://\"+dbhost+\":\"+dbport+\"/\"+dbname\n",
    "    properties = {\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"user\": dbuser,\n",
    "        \"password\": dbpass\n",
    "    }\n",
    "    #Format dataframe to match DB table\n",
    "    df = data_frame.withColumn(\"start_event_time\",data_frame['window'].start).withColumn(\"end_event_time\",data_frame['window'].end)\n",
    "    df = df.drop('window')\n",
    "    df = df.select(\"start_event_time\",\"end_event_time\",\"nr_of_events\",\"nr_items_viewed\",\"nr_items_put_in_cart\", \\\n",
    "                   \"nr_items_sold\",\"value_items_sold_in_cent\")\n",
    "    df.persist()\n",
    "    #send to PostgreSQL DB\n",
    "    df.write.jdbc(url=url, table=\"events\", mode=\"append\", properties=properties)\n",
    "    #send to Kafka\n",
    "    df.select(to_json(struct(\"start_event_time\", \"end_event_time\",\"nr_of_events\")).alias(\"value\")) \\\n",
    "      .write \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER) \\\n",
    "      .option(\"topic\", KAFKA_TOPIC_NAME_OUT) \\\n",
    "      .save()\n",
    "    \n",
    "    df.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Write aggregated metrics to PostgreSQL DB and Kafka \n",
    "query = queryDf \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", \"true\")\\\n",
    "    .foreachBatch(sink)\\\n",
    "    .trigger(processingTime = TIME_TRIGGER)\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}